import os
import json
from supabase import create_client, Client
from openai import OpenAI
from datetime import datetime

# --- CONFIGURACI칍N DE ACCESO (Asume Variables de Entorno en Railway) ---
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# Inicializar clientes
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Modelos
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4-turbo-2024-04-09" # O tu modelo preferido (Claude 3.5 Sonnet)

# --- 1. DEFINICI칍N DEL PROMPT MAESTRO ---

PROMPT_MAESTRO = """
Act칰a como un analista de negocios C-Level. Tu objetivo es generar un reporte ejecutivo diario
basado 칰nicamente en el CONTEXTO proporcionado de las conversaciones de WhatsApp del d칤a anterior.

INSTRUCCIONES DE FORMATO (Output):
- Utiliza formato Markdown.
- M치ximo 500 palabras.
- Tono: Formal, objetivo y conciso.

AN츼LISIS DE DATOS:

## I. Tareas Cr칤ticas y Acuerdos
1. **Acuerdos del D칤a:** (M치ximo 3 puntos). Las decisiones clave o compromisos de acci칩n.
2. **Problemas Bloqueantes:** (M치ximo 2 puntos). Obst치culos o escalamientos que requieren intervenci칩n.
3. **Riesgos Identificados:** (Un p치rrafo). Resumen de riesgos potenciales (ej: retrasos, fallas).

## II. Resumen Operacional
1. **M칠tricas Clave/Avance:** Citas de progreso de proyectos.
2. **Pr칩ximos Pasos:** (M치ximo 3 puntos). Tareas inmediatas pendientes.

---
REGLA DE ORO: Si no encuentras informaci칩n para una secci칩n, om칤tela o responde: "No se identificaron datos relevantes." NUNCA INVENTES.
"""

# --- 2. FUNCIONES DE CONSULTA RAG ---

def get_query_embedding(query: str) -> list[float]:
    """Genera el embedding del t칠rmino de b칰squeda usando OpenAI."""
    response = openai_client.embeddings.create(
        input=query,
        model=EMBEDDING_MODEL
    )
    return response.data[0].embedding

def get_context_from_db(query_embedding: list[float]) -> str:
    """Consulta la DB usando la funci칩n RPC match_messages."""
    
    # Par치metros para la funci칩n RPC
    params = {
        'query_embedding': query_embedding,
        'match_threshold': 0.78, # Umbral de similitud (ajustable)
        'match_count': 50,      # N칰mero de fragmentos a recuperar
        'time_limit_hours': 24  # 칔ltimas 24 horas
    }
    
    try:
        # Llamada a la funci칩n RPC que usa pgvector
        response = supabase.rpc('match_messages', params).execute()
        
        context_data = response.data
        
    except Exception as e:
        print(f"Error al consultar Supabase (RPC): {e}")
        return "ERROR: No se pudo obtener contexto de la base de datos."

    # Formatear el contexto para el LLM
    context_list = []
    for row in context_data:
        # Incluimos la similitud para fines de depuraci칩n
        sim = round(row.get('similarity', 0.0), 3)
        context_list.append(f"[Similitud: {sim} | {row['fecha_hora']} | {row['remitente']}]: {row['contenido_texto']}")

    if not context_list:
        return "SIN DATOS RELEVANTES: No se encontraron mensajes que coincidan con la b칰squeda RAG en las 칰ltimas 24 horas."
        
    return "\n---\n".join(context_list)

# --- 3. L칍GICA PRINCIPAL DEL REPORTE ---

def generate_daily_report():
    print("--- 游 Iniciando Generaci칩n de Reporte RAG ---")
    
    # 1. Definir el "t칠rmino de b칰squeda" para obtener un contexto amplio
    query_topic = "Resumen de acuerdos, problemas y avances del 칰ltimo d칤a de operaci칩n en los grupos de WhatsApp para reporte ejecutivo."
    
    # 2. Generar embedding de la consulta
    query_vector = get_query_embedding(query_topic)
    
    # 3. Obtener el contexto m치s relevante del pgvector (los 50 fragmentos clave)
    contexto_relevante = get_context_from_db(query_vector)
    
    if "ERROR" in contexto_relevante or "SIN DATOS RELEVANTES" in contexto_relevante:
        print(f"Abortando reporte. {contexto_relevante}")
        return f"Reporte fallido: {contexto_relevante}"

    # 4. Enviar el Prompt Maestro + Contexto al LLM
    try:
        prompt_final = f"{PROMPT_MAESTRO}\n\n--- CONTEXTO RECUPERADO DE LA DB ---\n{contexto_relevante}"
        
        # Enviar la solicitud a la API de OpenAI
        chat_completion = openai_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "Eres un analista ejecutivo experto, conciso y formal."},
                {"role": "user", "content": prompt_final}
            ],
            temperature=0.1,
        )
        reporte_final = chat_completion.choices[0].message.content
        
        print("\n\n=============== REPORTE FINAL GENERADO ===============")
        print(reporte_final)
        print("======================================================")
        
        # 5. Distribuci칩n (Aqu칤 a침adir칤as el c칩digo de env칤o de email o WhatsApp)
        # Ejemplo: distribute_report_via_email(reporte_final)
        
        return reporte_final
        
    except Exception as e:
        return f"Error al interactuar con el LLM: {e}"

if __name__ == "__main__":
    generate_daily_report()
